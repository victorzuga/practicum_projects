{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kmulmzlfebmkz3a531c8n"
   },
   "source": [
    "# Project for Wikishop with BERT\n",
    "\n",
    "Online store \"Wikishop\" launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities. That is, clients propose their edits and comment on the changes of others. The store needs a tool that will look for toxic comments and submit them for moderation.\n",
    "\n",
    "Train the model to classify comments into positive and negative. At your disposal is a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Build a model with a quality metric *F1* of at least 0.75.\n",
    "\n",
    "**Instructions for the implementation of the project**\n",
    "\n",
    "1. Download and prepare data.\n",
    "2. Train different models.\n",
    "3. Draw conclusions.\n",
    "\n",
    "It is not necessary to use *BERT* to run the project, but you can try.\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "The data is in the `toxic_comments.csv` file. The *text* column contains the text of the comment, and *toxic* is the target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ymh7d5ifx4pbka8gx4lv8"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "8avrqd32jsf0eotox9tn9kp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "fjg7z4dpchmcnwls2yh1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дубликаты: 0\n",
      "\n",
      "Пропуски\n",
      "text     0.0\n",
      "toxic    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"toxic_comments.csv\")\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "#function to check the data\n",
    "def data_check(data):\n",
    "    data.info()\n",
    "    print()\n",
    "    display(data.head())\n",
    "    print()\n",
    "    print('Duplicates:', data.duplicated().sum()) \n",
    "    print()\n",
    "    print('Missing values')\n",
    "    print(data.isna().mean())\n",
    "    \n",
    "data_check(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "m8wbem74vkhpkti5t8r5q"
   },
   "source": [
    "### Conclusions:\n",
    "* no duplicates or omissions\n",
    "* texts in English\n",
    "* need to remove regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "mobnoab9c8t6zven8fog9"
   },
   "outputs": [],
   "source": [
    "#reduce dataset size\n",
    "df = df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xd49e9g3wdz7hlmw02kko"
   },
   "outputs": [],
   "source": [
    "#function to clean up regular expressions and stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clear_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if not word.lower() in stopwords.words()]\n",
    "    return \" \".join(re.sub(r'[^a-zA-Z ]', ' ', ' '.join(text)).split())\n",
    "\n",
    "tqdm.pandas()\n",
    "df['text'] = df['text'].progress_apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "wdlkpm2mupg5ceo1ggs9mh"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3521b241086d443a8f0e1e66806a399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6ae02af02e4d37b0cf423af8cc6345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=442.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3449732942514382aabe1d957c8d4250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267967963.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fe3d7bb869461fb6571c7175a6e732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5000 5000\n",
      "4000 1000\n"
     ]
    }
   ],
   "source": [
    "#tokenizer initialization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "#converting text to token numbers from a dictionary\n",
    "tokenized = df['text'].apply(lambda x: tokenizer.encode(x, \n",
    "                                                        add_special_tokens=True, truncation=True, max_length=512))\n",
    "\n",
    "#padding\n",
    "max_len = 512 #max number of tokens for Bert\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "#creating a mask of tokens\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "#run on GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#model BERT\n",
    "model_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "#creating embeddings\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model_bert(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "\n",
    "#features and target       \n",
    "features = np.concatenate(embeddings)\n",
    "target = df['toxic'][:159500] #due to batch=100, some texts were not included in the features\n",
    "\n",
    "#sampling\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, \n",
    "                                                                            test_size=0.2, random_state=12345)\n",
    "\n",
    "#sample size check\n",
    "print(len(features), len(target))\n",
    "print(len(features_train), len(target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oivedev65jjzt92f1ub6c"
   },
   "source": [
    "## Education\n",
    "\n",
    "I will train two models: Logistic regression and Catboost.\n",
    "\n",
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "p5z5vy9zessvydda41u5z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на обучающей выборке: 0.6829376213030843\n",
      "F1 на тестовой выборке: 0.7358490566037736\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(StandardScaler(), LogisticRegression(solver='saga', C=0.1))\n",
    "rmse = cross_val_score(model, features_train, target_train, cv=3, scoring='f1')\n",
    "rmse = np.mean(rmse)\n",
    "\n",
    "model.fit(features_train, target_train)\n",
    "rmse_test = f1_score(target_test, model.predict(features_test))\n",
    "print('F1 на обучающей выборке:', rmse)\n",
    "print('F1 на тестовой выборке:', rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "idh0tzl8mceuk0dl3wozb"
   },
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "8kireclyful1cbb86ero5p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на обучающей выборке: 0.5905409782960804\n",
      "F1 на тестовой выборке: 0.6739130434782609\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(task_type=\"GPU\", verbose=False)\n",
    "rmse = cross_val_score(model, features_train, target_train, cv=2, scoring='f1')\n",
    "rmse = np.mean(rmse)\n",
    "\n",
    "model.fit(features_train, target_train)\n",
    "rmse_test = f1_score(target_test, model.predict(features_test))\n",
    "print('F1 на обучающей выборке:', rmse)\n",
    "print('F1 на тестовой выборке:', rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "owfekr2aedqixk5bfbktb"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "* models were trained to determine the toxicity of comments\n",
    "* classification was implemented on embeddings using BERT\n",
    "* using BERT is very resource intensive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notebookId": "700e377a-3334-496b-bb8b-7b83f418a547",
  "notebookPath": "notebook.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
